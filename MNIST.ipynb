{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number3 and 8:\n",
      "K-means {'Accuracy': 0.839, 'ARI': 0.459, 'AMI': 0.367}\n",
      "MeanShift {'Accuracy': 0.823, 'ARI': 0.484, 'AMI': 0.374}\n",
      "DBSCAN {'Accuracy': 0.347, 'ARI': 0.44, 'AMI': 0.387}\n",
      "AgglomerativeClustering {'Accuracy': 0.835, 'ARI': 0.45, 'AMI': 0.355}\n",
      "GMM {'Accuracy': 0.831, 'ARI': 0.439, 'AMI': 0.347}\n",
      "MBMM {'Accuracy': 0.832, 'ARI': 0.44, 'AMI': 0.371}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dart/peng/FBBMM.py:98: RuntimeWarning: divide by zero encountered in log\n",
      "  log_prob[n,c] = np.log(val)+self.log_gamma_function(np.sum(para))-np.sum(self.log_gamma_function(para))\n",
      "/home/dart/peng/FBBMM.py:96: IntegrationWarning: The integral is probably divergent, or slowly convergent.\n",
      "  val = integrate.quad(f, lower_bound, upper_bound, args=(x[0],x[1]))[0]\n",
      "/home/dart/peng/FBBMM.py:96: IntegrationWarning: The algorithm does not converge.  Roundoff error is detected\n",
      "  in the extrapolation table.  It is assumed that the requested tolerance\n",
      "  cannot be achieved, and that the returned result (if full_output = 1) is \n",
      "  the best which can be obtained.\n",
      "  val = integrate.quad(f, lower_bound, upper_bound, args=(x[0],x[1]))[0]\n",
      "/home/dart/peng/FBBMM.py:96: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n",
      "  If increasing the limit yields no improvement it is advised to analyze \n",
      "  the integrand in order to determine the difficulties.  If the position of a \n",
      "  local difficulty can be determined (singularity, discontinuity) one will \n",
      "  probably gain from splitting up the interval and calling the integrator \n",
      "  on the subranges.  Perhaps a special-purpose integrator should be used.\n",
      "  val = integrate.quad(f, lower_bound, upper_bound, args=(x[0],x[1]))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FBBMM {'Accuracy': 0.829, 'ARI': 0.433, 'AMI': 0.349}\n",
      "number1 and 7:\n",
      "K-means {'Accuracy': 0.971, 'ARI': 0.889, 'AMI': 0.813}\n",
      "MeanShift {'Accuracy': 0.975, 'ARI': 0.903, 'AMI': 0.832}\n",
      "DBSCAN {'Accuracy': 0.944, 'ARI': 0.857, 'AMI': 0.761}\n",
      "AgglomerativeClustering {'Accuracy': 0.973, 'ARI': 0.897, 'AMI': 0.823}\n",
      "GMM {'Accuracy': 0.97, 'ARI': 0.883, 'AMI': 0.806}\n",
      "MBMM {'Accuracy': 0.93, 'ARI': 0.738, 'AMI': 0.633}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dart/peng/FBBMM.py:96: IntegrationWarning: The algorithm does not converge.  Roundoff error is detected\n",
      "  in the extrapolation table.  It is assumed that the requested tolerance\n",
      "  cannot be achieved, and that the returned result (if full_output = 1) is \n",
      "  the best which can be obtained.\n",
      "  val = integrate.quad(f, lower_bound, upper_bound, args=(x[0],x[1]))[0]\n",
      "/home/dart/peng/FBBMM.py:96: IntegrationWarning: The integral is probably divergent, or slowly convergent.\n",
      "  val = integrate.quad(f, lower_bound, upper_bound, args=(x[0],x[1]))[0]\n",
      "/home/dart/peng/FBBMM.py:96: IntegrationWarning: The maximum number of subdivisions (50) has been achieved.\n",
      "  If increasing the limit yields no improvement it is advised to analyze \n",
      "  the integrand in order to determine the difficulties.  If the position of a \n",
      "  local difficulty can be determined (singularity, discontinuity) one will \n",
      "  probably gain from splitting up the interval and calling the integrator \n",
      "  on the subranges.  Perhaps a special-purpose integrator should be used.\n",
      "  val = integrate.quad(f, lower_bound, upper_bound, args=(x[0],x[1]))[0]\n",
      "/home/dart/peng/FBBMM.py:98: RuntimeWarning: invalid value encountered in log\n",
      "  log_prob[n,c] = np.log(val)+self.log_gamma_function(np.sum(para))-np.sum(self.log_gamma_function(para))\n",
      "/home/dart/peng/FBBMM.py:98: RuntimeWarning: divide by zero encountered in log\n",
      "  log_prob[n,c] = np.log(val)+self.log_gamma_function(np.sum(para))-np.sum(self.log_gamma_function(para))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FBBMM {'Accuracy': 0.976, 'ARI': 0.907, 'AMI': 0.841}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cluster, datasets, mixture\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import MBMM\n",
    "from MBMM import MBMM\n",
    "import FBBMM\n",
    "from FBBMM import FBBMM\n",
    "\n",
    "import random\n",
    "from sklearn.cluster import KMeans, MeanShift, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "## Calculate accuracy reference from: https://github.com/sharmaroshan/MNIST-Using-K-means  \n",
    "def infer_cluster_labels(pred_labels, actual_labels):\n",
    "    \"\"\"\n",
    "    Associates most probable label with each cluster in KMeans model\n",
    "    returns: dictionary of clusters assigned to each label\n",
    "    \"\"\"\n",
    "    \n",
    "    inferred_labels = {}\n",
    "    n_clusters = len(set(actual_labels))\n",
    "    #n_clusters = len(np.unique(pred_labels))\n",
    "    for i in range(n_clusters):\n",
    "\n",
    "        # find index of points in cluster\n",
    "        labels = []\n",
    "        index = np.where(pred_labels == i)\n",
    "\n",
    "        # append actual labels for each point in cluster\n",
    "        labels.append(actual_labels[index])\n",
    "\n",
    "        # determine most common label\n",
    "        if len(labels[0]) == 1:\n",
    "            counts = np.bincount(labels[0])\n",
    "        else:\n",
    "            counts = np.bincount(np.squeeze(labels))\n",
    "\n",
    "        # assign the cluster to a value in the inferred_labels dictionary\n",
    "        if len(counts) > 0:        \n",
    "            if np.argmax(counts) in inferred_labels:\n",
    "                # append the new number to the existing array at this slot\n",
    "                inferred_labels[np.argmax(counts)].append(i)\n",
    "            else:\n",
    "                # create a new array in this slot\n",
    "                inferred_labels[np.argmax(counts)] = [i]    \n",
    "\n",
    "    return inferred_labels  \n",
    "    \n",
    "    \n",
    "def infer_data_labels(X_labels, cluster_labels):\n",
    "    \"\"\"\n",
    "    Determines label for each array, depending on the cluster it has been assigned to.\n",
    "    returns: predicted labels for each array\n",
    "    \"\"\"\n",
    "    predicted_labels = np.array([-1 for i in range(len(X_labels))])\n",
    "    \n",
    "    for i, cluster in enumerate(X_labels):\n",
    "        for key, value in cluster_labels.items():\n",
    "            if cluster in value:\n",
    "                predicted_labels[i] = key\n",
    "                \n",
    "    return predicted_labels\n",
    "\n",
    "def load_data():\n",
    "    data = pd.read_csv('mnist_2d.csv').to_numpy() #70000,2\n",
    "    \n",
    "    (X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "    lower, upper = 0.01, 0.99\n",
    "    data = lower + (data - np.min(data))*(upper-lower)/(np.max(data)-np.min(data))\n",
    "    target = np.concatenate((Y_train, Y_test), axis = 0)\n",
    "    return data, target\n",
    "\n",
    "def data_to_target(data, target):\n",
    "    data_dict = {}\n",
    "    for i in range(len(data)):\n",
    "        if target[i] not in data_dict.keys():\n",
    "            data_dict[target[i]] = []\n",
    "            data_dict[target[i]].append(data[i])\n",
    "        else:\n",
    "            data_dict[target[i]].append(data[i])\n",
    "            \n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def smaple_number(num_list, data_dict):\n",
    "    data = []\n",
    "    target = []\n",
    "    total_train = 0\n",
    "    for n in num_list:  \n",
    "        data = np.append(data, data_dict[n])        \n",
    "        target = np.append(target, [n for _ in range(len(data_dict[n]))])       \n",
    "        total_train += len(data_dict[n]) #累積的資料數 \n",
    "    data = np.reshape(np.array(data), (total_train, 2))\n",
    "    target = np.reshape(np.array(target), (total_train)).astype(int)\n",
    "    \n",
    "    return data, target\n",
    "    \n",
    "\n",
    "def initial_param(data, target):\n",
    "    # ============\n",
    "    # Initial parameters\n",
    "    # ============\n",
    "    MBMM_param38 = np.array([[1.09717833, 8.04592969, 8.0686254 ],\n",
    "                             [2.84798971, 2.20653172, 1.52946912]])\n",
    "    \n",
    "    FBBMM_param38 = np.array([[3.14190044, 6.35378162, 4.94796333, 3.971114  ],\n",
    "                         [8.28131719, 8.89802408, 6.70568243, 3.18620078]])\n",
    "    \n",
    "    MBMM_param17 = np.array([[4.00195568, 9.09896669, 4.21570771],\n",
    "                                 [2.2397298,  2.91100834, 3.6114422 ]])\n",
    "    \n",
    "\n",
    "    FBBMM_param17 = np.array([[9.64563123, 6.51637147, 7.77768709, 3.65455795],\n",
    "                                 [2.15407161, 3.84410293, 4.04455582, 8.6491259 ]])\n",
    "    \n",
    "    data_dict = data_to_target(data, target)\n",
    "    \n",
    "    data_38, target_38 = smaple_number([3,8], data_dict)\n",
    "    data_17, target_17 = smaple_number([1,7], data_dict)\n",
    "     \n",
    "    param = [(data_38, {'n_clusters': 2, 'quantile': .22, 'eps': .16, 'min_samples': 4140,'linkage': \"ward\", \n",
    "                     'affinity': \"euclidean\", 'MBMM_param': MBMM_param38, 'FBBMM_param':FBBMM_param38}),\n",
    "            (data_17, {'n_clusters': 2, 'quantile': .35, 'eps': .11, 'min_samples': 2300, 'linkage': \"ward\", \n",
    "                       'affinity': \"euclidean\", 'MBMM_param': MBMM_param17, 'FBBMM_param':FBBMM_param17})]\n",
    "    target = [target_38, target_17]\n",
    "    \n",
    "    return param, target\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mnist_data, mnist_target = load_data()\n",
    "               \n",
    "    parameters, target = initial_param(mnist_data, mnist_target)\n",
    "\n",
    "    for i_dataset, (dataset, params) in enumerate(parameters):  \n",
    "\n",
    "        kmeans = cluster.KMeans(n_clusters=params['n_clusters'])\n",
    "               \n",
    "        bandwidth = cluster.estimate_bandwidth(dataset, quantile=params['quantile'])\n",
    "        ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "\n",
    "        dbscan = cluster.DBSCAN(eps=params['eps'], min_samples=params['min_samples'])\n",
    "\n",
    "        aggolmarative = cluster.AgglomerativeClustering(\n",
    "            linkage=params['linkage'],\n",
    "            affinity=params['affinity'],\n",
    "            n_clusters=params['n_clusters'],\n",
    "        )\n",
    "\n",
    "        gmm = mixture.GaussianMixture(n_components=params['n_clusters'], covariance_type='full')\n",
    "\n",
    "        mbmm = MBMM(n_components=params['n_clusters'], n_runs=100, param=params['MBMM_param'], tol=1e-3)\n",
    "        \n",
    "        fbbmm = FBBMM(n_components=params['n_clusters'], n_runs=20, param=params['FBBMM_param'], tol=1e-2)   \n",
    "\n",
    "        clustering_algorithms = (\n",
    "            ('K-means', kmeans),\n",
    "            (\"MeanShift\", ms),\n",
    "            (\"DBSCAN\", dbscan),\n",
    "            (\"AgglomerativeClustering\", aggolmarative),\n",
    "            ('GMM', gmm),\n",
    "            ('MBMM', mbmm),\n",
    "            ('FBBMM', fbbmm))\n",
    "        \n",
    "        #print result\n",
    "        if i_dataset == 0:     \n",
    "            print('number3 and 8:')\n",
    "        if i_dataset == 1:\n",
    "             print('number1 and 7:')\n",
    "                \n",
    "        for name, algorithm in clustering_algorithms:\n",
    "            algorithm.fit(dataset)\n",
    "\n",
    "            if hasattr(algorithm, 'labels_'):\n",
    "                train_predict_y = algorithm.labels_.astype(int)\n",
    "            else:\n",
    "                train_predict_y = algorithm.predict(dataset)        \n",
    "           \n",
    "            cluster_labels = infer_cluster_labels(train_predict_y, target[i_dataset])\n",
    "            train_predicted_labels = infer_data_labels(train_predict_y, cluster_labels)       \n",
    "            acc = np.round(np.count_nonzero(target[i_dataset] == train_predicted_labels)/len(target[i_dataset]), 3)\n",
    "\n",
    "            ari_value = np.round(metrics.adjusted_rand_score(target[i_dataset], train_predict_y), 3)\n",
    "\n",
    "            ami_value = np.round(metrics.adjusted_mutual_info_score(target[i_dataset], train_predict_y), 3)\n",
    "                   \n",
    "            print(name, {'Accuracy':acc, 'ARI':ari_value, \"AMI\":ami_value})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
